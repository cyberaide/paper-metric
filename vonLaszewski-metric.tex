\documentclass{sig-alternate} 

\newcommand{\TITLE}{Towards Understanding Cloud Usage with Statistical Analysis of Measured Data}
\newcommand{\AUTHOR}{Hyungro Lee, Gregor von Laszewski, Fugang Wang} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% LATEX DEFINITIONS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{hyperref} 
\usepackage{array} 
\usepackage{graphicx} 
\usepackage{booktabs} 
\usepackage{pifont} 
\usepackage{todonotes} 
\usepackage{rotating} 
\usepackage{color} 
 
\newcommand*\rot{\rotatebox{90}} 
 
\newcommand{\FILE}[1]{\todo[color=green!40]{#1}} 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% HYPERSETUP 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypersetup{ 
    bookmarks=true,         % show bookmarks bar 
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks 
    pdftoolbar=true,        % show Acrobat’s toolbar 
    pdfmenubar=true,        % show Acrobat’s menu 
    pdffitwindow=false,     % window fit to page when opened 
    pdfstartview={FitH},    % fits the width of the page to the window 
    pdftitle={\TITLE},    % title 
    pdfauthor={\AUTHOR},     % author 
    pdfsubject={Subject},   % subject of the document 
    pdfcreator={Gregor von Laszewski, Fugang Wang},   % creator of the document 
    pdfproducer={Gregor von Laszewski}, % producer of the document 
    pdfkeywords={hindex} {metric}{XSEDE} {FutureGrid}, % list of keywords 
    pdfnewwindow=true,      % links in new window 
    colorlinks=false,       % false: boxed links; true: colored links 
    linkcolor=red,          % color of internal links (change box color with linkbordercolor) 
    citecolor=green,        % color of links to bibliography 
    filecolor=magenta,      % color of file links 
    urlcolor=cyan           % color of external links 
} 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{document} 
% 
% --- Author Metadata here --- 
\conferenceinfo{TBD}{TBD Address} 
\CopyrightYear{2014}  
\crdata{X-XXXXX-XX-X/XX/XX}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE. 
% --- End of Author Metadata --- 
 
\title{\TITLE} 
%\subtitle{[Extended Abstract] 
%\titlenote{A full version of this paper is available as 
%\texttt{www.acm.org/eaddress.htm}}} 
 
\numberofauthors{4}  
\author{ 
\alignauthor 
Hyungro Lee\\ 
       \affaddr{Indiana University}\\ 
       \affaddr{2719 10th Street}\\ 
       \affaddr{Bloomington, Indiana, U.S.A.}\\ 
\alignauthor 
Gregor von Laszewski\titlenote{Corresponding Author.}\\ 
       \affaddr{Indiana University}\\ 
       \affaddr{2719 10th Street}\\ 
       \affaddr{Bloomington, Indiana, U.S.A.}\\ 
       \email{laszewski@gmail.com} 
\alignauthor 
Fugang Wang\\ 
       \affaddr{Indiana University}\\ 
       \affaddr{2719 10th Street}\\ 
       \affaddr{Bloomington, Indiana, U.S.A.}\\ 
% 3rd. author 
\and
\alignauthor 
Geoffrey C. Fox\\ 
       \affaddr{Indiana University}\\ 
       \affaddr{2719 10th Street}\\ 
       \affaddr{Bloomington, Indiana, U.S.A.}\\ 
\and  % use '\and' if you need 'another row' of author names 
} 
\date{13 March 2014} 
 
\toappear{} 
\maketitle 
\begin{abstract} 

In utility computing, usage data is necessary to identify utilization of the infrastructure by users. Many cloud platforms recently started to collect measurements for use of resources that can be applied to billing and monitoring. The usage data allows a user to see as how all resources are efficiently supplied to their applications and discover usage pattern in historical data. Virtual resources such as compute, storage and network are typically measured to evaluate time and cost of user applications and the statistics for resource used offer visibility to utilization of the cloud.

\end{abstract} 
 
% A category with the (minimum) three required fields 
\category{H.4}{Information Systems Applications}{Miscellaneous} 
%A category including the fourth, optional field follows... 
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures] 
 
\terms{Theory} 
 
\keywords{Scientific impact, bibliometric, h-index, Technology audit, XSEDE} 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% SECTIONS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\section{Introduction} 

In distributed systems and HPC, resource usage are typically monitored to detect any hardware and software issues. Real-time monitoring applications help provide sustain and consistent services and engage performance of their system. Distributed systems were built with complex hardwares and require to incorporate with various hardwares such as router, switch, network, and servers along with computing resources like cpu, memory and disk. In Cloud Computing, people have attention to monitoring and accounting systems in virtualized environments, so that they can measure resource consumption which is what they are paying for on the on-demand service, cloud computing. 

Real Consumption vs Allocation

There are two types of measuring resource usage on the cloud. Like a conventional monitoring, resource consumption on the cloud is based on current usage data for cpu, memory, network and disk traffics. These are dynamically changing according to traffics, and are important to validate system health frequently. The other type of measuring resource usage is measuring the amount of allocated resources. It is an accounting system that records allocation of resources. In a shared resource environment which is a fundamental concept in utility computing, the amount of allocated resource means that your requested resource will be dominated to you not interrupted by any other users. Resource allocation is not measuring real-time resource usage. Instead, it records rented resources in an accounting book for billing and charging. There are static metrics for allocation such as allocated number of cpu cores, memories, and disks. Number of public IP addresses is also counted for metrics.

\section{Usage Measurement on IaaS}

Openstack

In late 2012, OpenStack community started a new project about measuring usage data from openstack components. This project named Ceilometer collects measurements within OpenStack to achieve monitoring and metering purposes. Ceilometer acquires all of the measurements across all current OpenStack components such as Nova (compute), Network, and Storage (swift), etc and provides a unique framework for the collected data. The latest release Havana includes Ceilometer as a mandatory component in OpenStack and the previous release Grizzly included it as an incubator component.

OpenStack Compute (Nova) also provides a command line tools to retrieve usage statistics, for example, ‘nova usage-list’ provides usage data for all tenants. These management commands typically limited to system administrators to execute.

Ceilometer

Ceilometer project is a framework for monitoring and metering the OpenStack cloud and Ceilometer is a primary place to get access of all usage data in openstack components. It was mainly developed to charge customers as a billing system. Like other commercial cloud platforms, for example Amazon Web Services, these metrics are included, with an hour level granularity :

* Compute (Nova)

* instance type, availability zone

* cpu core

* memory size

* nova volume block device type and availability zone

* Network

* data transfer (in / out), availability zone

* external floating ip

* Storage (Swift)

* disk size used

* data in/out

Implementation

There is a program named an agent on each OpenStack node and aggregates information about virtualized resources. The agent on each nova compute node uses Linux virtualization API (libvirt) and Windows Management Instrumentation (wmi) to extract essential information from hypervisor. Some other agents harvest the data from iptables, swift proxy or the nova database, if additional information can be obtained through these external services.

There are four basic components to Ceilometer:

* Agent: runs on each compute node and polls for resources utilization statistics.

* Collector: runs on management servers to manage the message queues for data coming from the agent. Metering data are stored to the openstack data store directly and a notification message are delivered to the Openstack messaging bus once it is processed.

* Data store: is a place of collected data. It provides interaction with the collector and a api server.

* API server: runs on management servers to provide statistics about the measured data.

Figure 1. Architecture of openStack Ceilometer [architectureofopenstackceilometer]

An API server provides access to metering data in the database via a REST API. A central agent polls utilization statistics for other resources not tied to instances or compute nodes. There may be only one instance of the central agent running for the infrastructure. A compute agent polls metering data and instances statistics from the compute node (primarily the hypervisor). Compute agents must run on each compute node that needs to be monitored. A collector monitors the message queues (for notifications sent by the infrastructure and for metering data coming from the agents). Notification messages are processed, turned into metering messages, signed, and sent back out onto the message bus using the appropriate topic. The collector may run on one or more management servers. A data store is a database capable of handling concurrent writes (from one or more collector instances) and reads (from the API server). The collector, central agent, and API may run on any node. These services communicate using the standard OpenStack messaging bus. Only the collector and API server have access to the data store. The supported databases are MongoDB, MySQL, PostgreSQL, HBase and DB2 [ceilometerdatabase]; however, A dedicated host for storing the Ceilometer database is recommended, as it can generate lots of writes . Production scale metering is estimated to have 386 writes per second and 33,360,480 events a day, which would require 239 Gb of volume for storing statistics per month. [volumeofceilometer]

Openstack itself has notification systems built into the existing OpenStack components. Most usage data are collected from these notification systems. Ceilometer also requests metering messages from a pollster plugin using the ‘ceilometer.poll.compute’ namespace.

Ceilometer with OpenStack Heat for autoscaling

The OpenStack Orchestration program, Heat, provides an autoscaling service with Ceilometer like Amazon CloudFormation. OpenStack Heat scales VM capacity up or down according to the metrics from Ceilometer. Ceilometer collects metrics for virtual machines and its alarming module calls the Heat API if the threshold for the metrics is reached. Heat triggers the upscaling or the downscaling virtual machines once it is notified by Ceilometer. This integration of Heat and Ceilometer allows you to ensure optimal utilization by managing the number of virtual machine instances. Amazon has a similar combination of AWS Auto Scaling and AWS CloudWatch to provide the autoscaling service based on monitoring values. [autoscalingwithheatandceilometer]

Ceilometer on Horizon

Horizon dashboard, a web-based graphical user interface, adds a new panel for Ceilometer in OpenStack Havana through the admin panel. Figure 2 to 4 show how they are displayed on the web.

Figure 2. Screenshot of ceilometer for disk usage

Figure 3. Screenshot of ceilometer for network usage

Figure 4. Screenshot of ceilometer for visualization

[ceilometeronhorizon]

Nova command line tools for usage statistics

Openstack provides usage statistics for OpenStack Compute (Nova), a main component for provisioning and managing virtual machines, with command-line tools. Simple commands displays basic statistics on resource usage for hosts (physica nodes) and instances (virtual objects running on the host). Basic information such as CPU, memory, and disk usage are viewed. These information about allocated resources to the instances do not indicate resource usage on the physical host. For more detailed information about resource usage, Ceilometer has rich functions to see user related or system related usage data. Ceilometer is available on OpenStack Hanava and Grizzly version.

Nova command line tools for usage statistics

Openstack provides usage statistics for OpenStack Compute (Nova), a main component for provisioning and managing virtual machines, with command-line tools. Simple commands displays basic statistics on resource usage for hosts (physica nodes) and instances (virtual objects running on the host). Basic information such as CPU, memory, and disk usage are viewed. These information about allocated resources to the instances do not indicate resource usage on the physical host. For more detailed information about resource usage, Ceilometer has rich functions to see user related or system related usage data. Ceilometer is available on OpenStack Hanava and Grizzly version.

Example 1. Display a summary of resource usage of the devstack-grizzly host

$ nova host-describe devstack-grizzly +------------------+----------------------------------+-----+-----------+---------+ | HOST | PROJECT | cpu | memory_mb | disk_gb | +------------------+----------------------------------+-----+-----------+---------+ | devstack-grizzly | (total) | 2 | 4003 | 157 | | devstack-grizzly | (used_now) | 3 | 5120 | 40 | | devstack-grizzly | (used_max) | 3 | 4608 | 40 | | devstack-grizzly | b70d90d65e464582b6b2161cf3603ced | 1 | 512 | 0 | | devstack-grizzly | 66265572db174a7aa66eba661f58eb9e | 2 | 4096 | 40 | +------------------+----------------------------------+-----+-----------+---------+

Usage data can be provided by Tenant Id which is a group of openstack cloud users. Each tenant id represents a group or an account to the group members, so usage data for the tenant id are aggregated.

Example 2. Summary statistics for tenants

$ nova usage-list Usage from 2013-06-25 to 2013-07-24: +----------------------------------+-----------+--------------+-----------+---------------+ | Tenant ID | Instances | RAM MB-Hours | CPU Hours | Disk GB-Hours | +----------------------------------+-----------+--------------+-----------+---------------+ | b70d90d65e464582b6b2161cf3603ced | 1 | 344064.44 | 672.00 | 0.00 | | 66265572db174a7aa66eba661f58eb9e | 3 | 671626.76 | 327.94 | 6558.86 | +----------------------------------+-----------+--------------+-----------+---------------+

[usagestatistics]

Usage data for Ceilometer and the nova command line tools is provided by OpenStack Notification System. The notification system can be configured to emit events either through nova's logging facility, or send them to a series of AMQP queues (one per notification priority). System usages are emitted as notification events with the INFO priority. Different types of usage events are distinguished via the notifications' event_type, which is a hierarchical dotted string such as compute.instance.create, which allows usages to be easily grouped for aggregation. Usage notifications can be immediate, created when a specific increment of usage occurs (such as creation of an instance) or periodic, generated by a periodic task, like a cron job, and covering usage for a certain period of time. Besides the standard Nova Notification priority, notification timestamp, and event_type, usage notifications contain a payload of data that will vary depending on the event_type. This is presented as a json-formatted hash of key-value pairs. Some of the keys, such as tenant_id will always be present in any usage notification, others will be data relevent to that event_type (For example, instance related notifications will contain data describing the instance.)

[systemusagedata]

Eucalyptus

The Eucalyptus Amazon compatible private cloud has provided resource usage information through external monitoring tools such as Nagios and Ganglia. Since both Nagios and Ganglia have been proved to observe state data within distributed systems, Eucalyptus relied on integration with these tools for resource monitoring. To enhance system management, Eucalyptus recently improves summary reports about resource allocation and status. There are commands line tools for generating reports for eucalyptus cloud that start with eureport- in the Cloud Controller (CLC) and eucadw- in the data warehouse. The reports provide usage data for understanding how cloud resources are utilized and being used via simple command line tools. eureport-generate-report is a main command to get access usage data. Various type of resources can be measured such as elastic-ip, instance, s3, snapshot, and volume when eureport-generate-report is ran with a report type option. The Eucalyptus data warehouse is a place to keep all usage data coming from CLC. External programs can get access to the usage data from the data warehouse instead of CLC directly. It may reduce impact of pulling usage information from cloud when it performs its cloud duties.

[managing-reporting] [eureport-] [eureport-generate-report]

Regarding to commercial clouds, usage data is provided to cloud services running under your account.

Azure

Windows Azure has enhanced their Azure Management Portal, a web-based graphical user interface, with monitoring panels.

-customize monitoring displays,

- verbose monitoring data, (usage data is stored in a storage account which you can access outside of the portal.)

- plot in metrics charts on the monitor page and the dashboard.

Performance Counters provides information about status of operating system, application, service or driver. Counter data can be used to determine resource consumption. Performance Counters is mainly used for monitoring which is gathered from the host operating system for the roles instances (vms). CPU usage(CPU percentage), Network traffic (data in/out), and disk usage (Read throughput, write throughput) are typically collected for minimal monitoring. This is also called performance data.

[howtomonitorcloudservicesazure]

The following diagram illustrates how consumers, the registry, PDH, and performance DLLs and application providers work together.

Figure 5. Overview of Performance Counters

Consumer A uses the registry interface to obtain counter information. Consumer B and the Performance Monitor use PDH to obtain counter information. In turn, the PDH functions can use either the registry interface or WMI.

[AboutPerformanceCounters]

Amazon

Amazon CloudWatch

“Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real-time.”

- Keyword: real-time and monitor resources and applications both.

how do they monitor resources and applications? does the applications mean that a specific applications? such as Elastic Map Reduce (EMR)?

“ACW is basically a metrics repository. AWS product puts metrics into the repository, and you retrieve statistics based on those metrics.

Metric - is a variable you want to measure for your resources and applications.

* New terms; Namespaces, dimensions, Metrics

Namespaces are containers for metrics.

Metrics are time-ordered sets of data points, are isolated from one another in different namespaces so that metrics from different applications are not mistakenly aggregated into the same statistics.

think of a metric as a variable to monitor, and the data points represent the values of that variable over time.

You retrieve statistics about those data points as an ordered set of time-series data.

Over the time value is important for metrics since it contains historical changes in it. Timestamp always follows with a metric.

Amazon provides PutMetricData API to create a custom metrics and publish to ACW.

2 weeks time period for store statistics.

Each metrics has a dimension, which is a name / value pair that helps you to uniquely identify a metric.

Alarm!

CloudWatch has a notification to alert users and auto scaling (automatically make changes) to the resources you are monitoring based on rules that you define. Simply, CloudWatch manages threshold values to send a notification to users via email or text messages, and even more, apply changes with a pre-defined settings such as increasing virtual instances or diminishing.

built-in metrics vs custom metrics

“You gain system-wide visibility into resource utilization, application performance, and operational health”

key concepts and terms

We need to define

- resources

- applications

- monitor

- alert

- automatic

Amazon DevPay

Amazon DevPay is a reseller model which provide a billing and account management service on top of Amazon Web Services (AWS). Amazon DevPay users can measure usage of AWS services, send a bill and collect payments from customers under their subscriptions. DevPay helps a process of billing and tracking for AWS services like Compute (EC2) and Storage (S3).

[AmazonDevPay][resellaws]

Figure 6. Billing Service - Amazon DevPay

Monitoring in HPC

Monitoring in high-performance computing has a similarity to Cloud Computing. It provides fixed number of jobs that a user can create and each job runs on a same size of nodes in clusters. There are also common management tools to clusters, so accounting data can be collected from these tools.

XDMoD

XDMoD (XSEDE Metrics on Demand) is primarily developed as UBMoD (University Buffalo Metrics on Demand) and is an open source tools for collecting and mining statistical data from cluster resource managers such as Torque/Maui, OpenPBS, SGE and Slurm commonly found in high-performance computing environments. There are three fundamental components: a metrics repository (XDMoD Data Warehouse), a RESTful API, and a web-based application (XDMoD Portal). Its web graphical user interface, XDMoD Portal, provides rich set of statistics with different type of charts and tables with communicating its RESTful API.

[Performance metrics and auditing framework using application kernels for high-performance computer systems]

[Using XDMoD to Facilitate XSEDE Operations, Planning and Analysis]


\section{Related Work} \label{S:related}
 

 
\section{System Design} \label{S:design}

  
 
\section{Implementation} \label{S:implementation}

TBD
\section{Results and Analyses} \label{S:result}

TBD
 
 
\section{Summary}

TBD 

\appendix{FROM OLD PAPER}

\section{tables}

use
\begin{verbatim}
p{2cm} 
\end{verbatim}
to replace some of the l's in the table, read up what p does, its
width of column


\begin{table*}[htb]
\caption{table 1}\label{T:tab1}
\begin{tabular}{lllll}
Service & Cost Estimate & Pricing (Small/Medium/Large) & Restriction \\
AWS & \$2 & 668.74  & \$0.06 \$0.12 \$0.24 per hour (US East Region; Linux) & Hour basis charge\\
GCE & \$2 & 231.54  & \$0.054 \$0.104 \$0.207 per hour (US Region; Linux) & Minute basis charge with 10-minute minimum\\
Azure & \$2 & 580.03  & \$0.06 \$0.12 \$0.24 per hour (Linux) & Minute basis charge with 5 less minute free\\
\end{tabular}
\end{table*}

\begin{table*}[htb]
\caption{table 2}\label{T:tab2}
\begin{tabular}{llllll}
Instance types & Instance count & Hour basis & Minute basis & With Google 10-minute minimum charge & with Azure 5-minute free \\
small & 165 & 619 hrs (37,140 mins) & 29,622 mins & 29,875 mins &29,582 mins \\
medium & 6 & 268 hrs (16,080 mins) & 15,891 mins & 15,891 mins & 15,891 mins\\
large & 490 & 10,831 hrs (649,860 mins) & 629,969 mins & 631,047 mins & 629,667 mins\\
Total & 661 & 11,718 hrs (703,080 mins) & 675,482 mins & 676,813 mins & 675,140 mins\\
\end{tabular}
\end{table*}


\begin{sidewaystable*}
\caption{table 2}\label{T:tab2}

\begin{small}
\begin{tabular}{|p{2cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{2cm}|p{1cm}|p{1.5cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
Provider & Charging & Cost 1vCPU/hour	   & Cost 1GB/hour & OS & Max vCPU & Memory min - max &\# of instance types & Discount program & Free allowance\\
\hline
\hline
Aws & hourly & \$0.04  & \$0.02  & Linux & 32 & 615MB - & 22 & spot instance;  & \$100 for educators and student\\
    &  	     &         &  	 & Windows +14-56\%   	&    & 244GB    &       & reserved instances & Grant for researcher, AWS educated grant program\\
    &  	     &         &  	 & Asia + 25\% 		&    & 		       &       &  	  	    & \\
\hline
Google Compute Engine & 10 minutes + & \$0.08  & \$0.01  & Linux (Debian; CentOS) & 16 & 600MB - & 1- & n/a & Google app reward programs\\
 & every minute after that &  &  & (RHEL; SUSE premium operating systems) *** &  & 104GB & (4 high cpu + 4 high memory + 2 small + 5 standard) &  & \$1000 for educator\\
 &  &  & Europe + 4.5\% - 27\%  &  &  &  &  &  & \$60;000 for research project\\
\hline 
IBM CloudLayer (by Softlayer) & monthly & \$0.50  & varies & Linux;  & 16 & 1GB  & Build your own cloud server offers customized options &  & one month trial for 1 vcpu + 1gb memory + 25 storage\\
\hline
 & hourly & to &  & Windows + \$0.05 to &  & -  &  &  & \\
 &  & \$0.10  &  &               \$0.10 / hour &  & 64GB &  &  & \\
\hline 
HP cloud & hourly & \$0.02  & \$0.02  & Linux; Windows; SUSE & 16 & 1GB  & 11 (8 standard + 3 memory intensive) &  & \$300 free trial for 90 days (\$100 for each month)\\
 &  &  &  & (windows: 10-200\% extra charge; &  & -  &  &  & \\
 &  &  &  & SUSE: 4\% - 200\% extra charge) &  & 120GB &  &  & \\
\hline 
Microsoft Azure & Free first 5 minutes & \$0.05  & \$0.02 (approx.) & Linux;  & 8 & 768MB -  & 8 (A0-A7) & 6-Month; 12-month pre-pay membership & \$200 free trial of first month\\
 &  &  & Windows is expensive 30-50\% more than linux & Windows + 30-50\%  &  & 56GB &  &  & \\
\hline 
Rackspace & minute &  & varies & Linux; Windows, (windows: 25\% extra charge) & 32 & 1GB - 120GB  & 9 & Volume discount (4\% to 20\% for spending over \$5;000 - \$ 10; 000 per month; 8\% for \$10;001 - 30;000; 12\% for \$30;001 - \$50;000) Commitment discount (4\% to 40\%) Prepayment discount with commitment (7\% to 55\%)& \$300 developer discount (\$50 each for six months)\\
\hline
\end{tabular}
\end{small}
\end{sidewaystable*}


\section{Images}



\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig1.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig1} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig2.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig2} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig3.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig3} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig4.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig4} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig5.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig5} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig6.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig6} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig3b.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig7} 
\end{figure} 

\begin{figure}[htb] 
  \centering 
    \includegraphics[width=1.0\columnwidth]{images/fig4b.pdf} 
  \caption{The Architecture of the Framework}\label{F:fig8} 
\end{figure} 


\section{BIG DATA}

With increasing the size of stored information in a rate of 23% per a year [20], the growth in the amount of educational and academic data means that data analysis, management and accessibility are going to be complicated to work with traditional data processing platforms and applications. Big Data is not only applied to the volume of data, but also applied to the length of the data life cycle [21] and variety, velocity, and veracity of the data [22]. In many areas such as meteorology, genomics, biological and environmental research [23, 24], researchers and educators need to process vast amounts of complex data instantly and accurately with analytical challenges. For example, Amazon Web Services and NASA work together to enable a large collection of NASA climate and Earth science satellite data to researchers and educators through Public Data Sets[?]. AWS Public Data Sets program is a centralized repository of large public data sets in an Elastic Block Storage or a Simple Storage Service (S3) format[?]. With the EBS snapshots, NASA NEX which is a large Earth science data sets can be easily attached to Amazon services i.e. Amazon Elastic Compute Cloud (Amazon EC2) for data analysis. 1000 human genomes to reveal DNA polymorphisms is also available through Amazon S3. The data sets have grown from small gigabytes to 200 terabytes.

\section{CAPABILITIES}

What is not met.

\section{USE CASE EXAMPLES}

\section{FutureGrid}

Based on the observation on FutureGrid, there is a different pattern between a research project and class work when they acquire cloud resources.  Resource allocation of academic coursework shows time dependent request patterns. It shows a surge when there is a class, a lab session, and a project. For example, the undergraduate course for Distributed Systems at Indiana University introduced IaaS in the class and used the IaaS platform for a class project. Figure 1 shows a spike in the class and variability until the project due. Research projects request VM instances in a bit more steady usage compared to the coursework. The Next Generation Sequencing (NGS) in the cloud project on FutureGrid shows relatively consistent resource allocation requested in Figure 2. With a certain period of time, vm instances of this project have been launched without unplanned spike requests. These examples show different patterns for requesting resources but both cases have a factor to predict loads. The class schedule and the monitoring and profiling data for applications can be used to measure the amount of resources and identify incoming requests. In paid cloud platforms such as AWS, GCE, and Azure, understanding these patterns for provisioning is important to bring cost effectiveness over on-demand allocation. For example, Amazon EC2 Reserved Instances and Azure pre-pay plans may help reduce usage costs for periodic and planned workloads. These service plans simply provide discounts with an upfront payment. As long as a class and a project go as planned, cost saving chances are increased. 

Figure 1. IaaS Usage data for the Distributed System class at Indiana University*

% * Based on the class schedule and metrics. Class schedule is here: http://salsahpc.indiana.edu/csci-p434-fall-2013/
% Metrics is here: http://129.79.49.94/accounting/reports/custom/p434fall13/FGResourceReport.pdf

 
Figure 2. VM count for Next Generation Sequencing (NGS) in the cloud project

With the Gantt chart in Figure 3, allocation activities are viewed for all virtual instances launched for the class. At the beginning of the class, the gaps between the start and completed dates of the vm instances are small but a large number of instances are initiated. Once the class is became operative,  running time for vm instances is getting longer and a smaller number of instances are requested compared to the beginning. This observation tells that academic projects require training sessions at the beginning of the project to get familiar with using infrastructure and to prepare environments by installing software and datasets.

 
Figure 3. Timeline for VM walltime

Other observation is that resource usage for administrative purposes. Figure 4 describes that instructors consumed a large number of vCPU cores before class starts and small tests just before class projects. It indicates that the preparation of courses require extensive load testing on cloud resources to estimate compute capacity needed for applications.

 
Figure 4. Usage between instructors and students for vCPU cores

During the semester, 25 hosts, 216 vCPUs and 600GB memories were reserved for the class since it required large virtual instances. In Figure 5 shows that the dedicated resources were being underutilized most time although the high volume requests had been made a few times including 273% overutilization on October 21th for testing and preparing.
 
Figure 5. vCPU Utilization (approximation per hour)

\section{OpenScience Grid}

\section{PlanetLab}

\section{COMPARISON}

QoS


% Overprovisioning
% Provider	Charging	Cost
% 1 vCPU /hour	Cost
% 1 GB
% /hour	OS	Max vCPU	Memory min - max	\# of instance types	Discount program	Free allowance
% Aws
% 	hourly	\$0.04
% 	$0.01927	Linux
% Windows +14-56% 
% Asia + 25%	32	615MB -
% 244GB	22	spot instance, 
% reserved instances	$100 for educator’s student
% Grant for researcher
% AWS educated grant program
% Google Compute Engine	10 minutes +
% every minute after that	$0.0788
% 	$0.006636
% 
% Europe + 4.5% - 27% 	Linux (Debian, CentOS)
% (RHEL, SUSE premium operating systems) ***	16	600MB -
% 104GB	15
% (4 high cpu + 4 high memory + 2 small + 5 standard)	n/a	Google app reward programs
% $1000 for educator
% $60,000 for research project
% IBM CloudLayer (by Softlayer)	monthly
% hourly	$0.5
% to
% $0.10	varies	Linux, 
% Windows + $0.05 to
%               $0.10 / hour	16	1GB 
% – 
% 64GB	Build your own cloud server offers customized options		one month trial for 1 vcpu + 1gb memory + 25 storage
% HP cloud	hourly	$0.015	$0.015	Linux, Windows, SUSE
% (windows: 10-200% extra charge,
% SUSE: 4% - 200% extra charge)	16	1GB 
% – 
% 120GB	11 (8 standard + 3 memory intensive)		$300 free trial for 90 days ($100 for each month)
% Microsoft Azure	Free first 5 minutes
% 	$ 0.05	$0.02 (approx.)
% Windows is expensive 30-50% more than linux	Linux, 
% Windows + 30-50% 	8	768MB – 
% 56GB	8 (A0-A7)	6-Month, 12-month pre-pay membership	$200 free trial of first month
% Rackspace	minute		varies	Linux, Windows
% (windows: 25% extra charge)	32	1GB – 
% 120GB	9	Volume discount (4% to 20% for spending over $5,000 - $ 10, 000 per month, 8% for $10,001 - 30,000, 12% for $30,001 - $50,000)
% Commitment discount (4% to 40%)
% Prepayment discount with commitment (7% to 55%)	$300 developer discount ($50 each for six months)
% Outage
% Expectations on quality

Comparison between iaas platforms?
Show changes from euca to openstack?

\section{FOR PAY ALLOCATIONS}

It has been made clear by the many cloud providers that the use of cloud data centers provides an economical advantage for the organization as the overall cost model that not only includes the 

\section{WHICH CLOUDS TO CONSIDER?}

\subsection{IaaS Cloud EC2 like services}
Charge. 
1.	find refernces and include here (properly, academic refernces, with pages to wher stuff is charged
2.	identify similarities and differences. For example google charges 15 min and after that every minute, AWS charges for an hour even if you just use a second
3.	

1)	AWS
*	http://aws.amazon.com/tco-calculator/
*	http://calculator.s3.amazonaws.com/calc5.html
*	http://aws.amazon.com/pricing/
*	spotpricing



\subsection{Pricing Comparison in IaaS}

Comparing pricing of the cloud is complicated and may lead to false analogy because each cloud provider offers various services with different performance. The pricing comparison, however, is important when people start to consider adopting cloud services among a lot of selections from different providers. In the comparison, important criteria are revealed through its pricing table. For example, there are a range of service offered, a size of available systems, costs, discounts and benefits such as technical support, and development tools. Amazon AWS, Windows Azure, Google Compute Engine, HP Cloud, IBM and Rackspace are compared.

 

 

 

\subsection{Example of pricing comparison}

We tried to apply each pricing model; Amazon AWS, Google Compute Engine, Microsoft Azure; to the usage data of class (P434 distributed systems at Indiana University), to compare cost estimate of cloud resource. Google Compute Engine is the least expensive and 16\% lower than Amazon AWS. It is mostly because of that Google has 10\% discount pricing chart compared to AWS. We observed that a minute basis charge is only 3.3\% less expensive for this class. Some restrictions and offers such as Google’s 10-minute minimum charge and Azure’s less 5-minute free of charge are relatively small amount of a discount or an extra charge. Google’s 10-minute minimum charge asks 0.18\% extra charge to the class, Azure provides 0.05\% discount through their less 5-minute free of charge. Amazon only has an hourly based pricing model, while Google Compute Engine and Windows Azure offer a minute basis charge for use of virtual machine instances. Three types of instances (small/medium/large) had been used for its coursework and projects and usage of virtual machine instances was only calculated without network and storage usage. Table 3, 4 shows pricing comparison to the class.

Table 3. Usage data of the class

Instance types	Instance count	Hour basis	Minute basis	With Google 10-minute minimum charge	with Azure 5-minute free
small	165	619 hrs (37,140 mins)	29,622 mins	29,875 mins	29,582 mins
medium	6	268 hrs (16,080 mins)	15,891 mins	15,891 mins	15,891 mins
large	490	10,831 hrs (649,860 mins)	629,969 mins	631,047 mins	629,667 mins
Total	661	11,718 hrs (703,080 mins)	675,482 mins	676,813 mins	675,140 mins
* Instance types are not same. Chosen by a similarity of vCPU and Memory
** Captured by January, 2014

Table 4. Pricing comparison to the class
Service	Cost Estimate	Pricing (Small/Medium/Large)	Restriction
AWS	$2,668.74	$0.06\$0.12\$0.24 per hour (US East Region, Linux)	Hour basis charge
GCE	$2,231.54	$0.054\$0.104\$0.207 per hour (US Region, Linux)	Minute basis charge with 10-minute minimum
Azure	$2,580.03	$0.06\$0.12\$0.24 per hour (Linux)	Minute basis charge with 5 less minute free

Charging model describe in text how they cjhharge and for what, e.g. what do they offer

1)	HP Cloud

2)	Google
3)	Rackspace
4)	IBM
5)	IBM

heruko vs amazon
% http://www.smashingboxes.com/heroku-vs-amazon-web-services/


Supported by libcloud
*	http://www.abiquo.com
*	http://brightbox.com
*	https://www.gandi.net/hosting/iaas/livemigration
*	http://www.vr.org
*	https://www.linode.com
*	https://www.bluebox.net
*	http://www.cloudsigma.com
*	https://www.digitalocean.com
*	dreamhost
*	http://www.elastichosts.com
*	gogid
*	gridspot
*	hostvirtual
*	ibm\_SCE
*	joynet
*	ktucloud
*	ninefold
*	opsource
*	rimuhosting
*	serverlove
*	skalicloud
*	slicehost
*	softlayer
*	voxel
*	vpsnet
*	bitnami?
*	
\subsection{Comparison}

Table 1. Pricing chart for instances from AWS, GCE, and Azure
	Billing granularity	Price	Variation for Price
AWS	By hour	\$0.02 (smallest)*	10 regions**, 6 platforms
GCE	By minute, with a minimum of 10 minutes	\$0.019 (smallest)*	2 regions (Us, Europe)
Azure	By minute, No minimum, No billing for less than 5 minutes	\$0.02 (smallest)*	6 regions, 5 platforms
* Smallest VM instance: 1 virtual core, 600~768MB memory, no storage
** China (Beijing) region will be available in early 2014, and GovCloud region is also included.

Pricing is scenario based. It can’t be simply compared with numbers. GCE looks cheaper than other competitors, but others have more options to save a cost. For example, a pay-ahead model provides a discount for same instances, and a spot instance also provides a way of saving entire cost for task intensive workloads in a small amount of time.


\subsection{Tools for Cloud Pricing}

To help understand costs of cloud computing, several tools offer various functions to calculate estimate and analyze cloud spending and usage data. With these tools, researchers and educators can make a plan for cloud deployments and prepare spending across several cloud providers. Each cloud service consists of complicated architectures and different performance levels and so users need to understand that these tools for cloud pricing are designed to minimize costs and provide guidelines in terms of finding the most effective cloud services.

1. PlanForCloud Calculator
PlanForCloud Calculator is a free cloud cost forecasting website that
helps evaluate costs for a project or an operation from a variety of
cloud providers. With PlanForCloud, Amazon Web services (AWS), Google
Compute Engine (GCE), Microsoft Windows Azure, Rackspace, IBM
Softlayer, and HP Cloud are compared for servers, storage, databases,
data transfer, and other services to estimate resource usage in the
future. This calculator also provides a simulation on planned
deployments and 3-year cost reports. Estimated expenditures can be
illustrated to reduce uncertainty for the future consumption of cloud
resources. 

% [http://www.rightscale.com/news_events/press_releases/2012/rightscale-introduces-cloud-cost-forecasting-with-launch-of-planforcloud.php]

[Helen Heinrich (2013) Tech Servises on the Web: The PlanforCloud
Calculator http://www.planforcloud.com/ , Technical Services Quarterly, 30:1, 120-121, DOI:
10.1080/07317131.2013.735980]

 
Figure 3. Example of a cloud usage report by PlanForCloud from RightCcale **

** screenshot from http://www.rightscale.com/images/pfc-report-page.jpg


 
Figure 4. Example of Cloud Analytics from Rightscale***

***screenshot from http://www.rightscale.com/images/screenshots/cloud-cost-forecasting.png

We can not compare all the things, so we need to identify a subset for comparision, what should we compare

Tiny, small, medium, lareg mage of comparable size

Create graph with entry for each type with y axis on some metric we can compare this will create a “band” on y but have a single entry for each IaaS. 

goalsCan accost model be derived for each cloud given some parameters



*	http://www.profitbricks.com/compare-cloud-pricing/
%*	https://cloudvertical.com/cloud-costs#cloud_costs/index
*	http://mikekhristo.com/ec2-ondemand-vs-reserved-instance-savings-calculator/
*	http://blog.cloudphysics.com/blog/2013/11/18/do-hybrid-clouds-make-cents-free-cost-calculator-for-aws
*	http://geekswithblogs.net/hroggero/archive/2013/02/18/sample-pricing-comparison-amazon-aws-and-windows-azure.aspx
*	http://www.stratalux.com/2013/05/08/strataluxs-aws-pricing-tool/
*	http://www.webrmedia.com/blog/apps/calculate-your-amazon-aws-hosting-costs-using-excel

D.	PaaS
1)	AWS service xyz
2)	???




\section{Tools}

1)	Management Tools
Rightscale
2)	Metric systems
Rightscale, 
….pulbic clouds that monitor your AWS or other cloud


\section{Citations}

just to show whats in cyberaide-metrics.bib

\cite{www-c,www-b,www-a,www-abc,www-salsa-class,www-amzon-calculator2,www-o,www-n,www-m,www-l,www-k,www-j,www-i,www-h,www-g,www-f,www-w,www-v,www-u,www-t,www-s,www-r,www-q,www-p,www-e,www-d,www-amzon-calculator-1,www-amzon-calculator-2}
 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Acknowledgment 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section*{Acknowledgement} 
 
This material is based upon work supported in part by the National Science Foundation under Grant No. 0910812.

 
%\clearpage 
 
\bibliographystyle{IEEEtranS} 
%\bibliographystyle{abbrv} 
\bibliography{% 
bib/vonLaszewski-jabref,% 
bib/cyberaide-metric,%
bib/cyberaide-cloud}
%bib/image-refs,% 

%bib/python,% 
%tas.bib%
 
\end{document} 
 
 
